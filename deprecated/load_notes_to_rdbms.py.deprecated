#!/usr/bin/env python3

# Import the data using Python.
# We could also adapt the OMOP bash script, but this is cross database
# TODO - schema support

# Ugh. This really should load CSVs
# psql \copy schema.table FROM output/allergies.csv WITH CSV

import os
import re
#import d6tstack
import pandas as pd
import sqlalchemy
from sqlalchemy import create_engine
from sqlalchemy import text
from datetime import datetime

def main(uid = 'postgres', pwd = '', host = 'localhost', db = 'syntheticMGUH',
         synthea_data_path = '../output', ehr_launch_date = datetime.strptime('2010-01-01', '%Y-%m-%d'),
         include_notes = False):
    #
    # uid = 'postgres'
    # pwd = ''

    # host = 'localhost'
    # db = 'syntheticMGUH'
    # synthea_data_path = '../output'
    # start = datetime.strptime('2010-01-01', '%Y-%m-%d')
    # include_notes = False

    # Synthea exports CSV without quotes. Some DC address lines are bad and contain commas due to bad addresses
    # eg "1234 Connecticut Ave NW, Washington DC, DC, 23007"
    # You will need to hand edit these or write a better regexp expression than mine.
    # This is because the Sythnea lookup tables in the source use quotes in the CSV files, but the
    # export files do not
    index_columns = set(["id", "patient", "encounter", "provider",
                         "payer", "organization", "date", "start", "stop"])

    print(f"importing from folder {synthea_data_path}")

    db_connection_string = f'postgresql+psycopg2://{uid}:{pwd}@{host}/{db}'
    db_engine = create_engine(db_connection_string)

    print("... loading notes")
    regex_id =  "(^.*_)(.*?).txt"
    regex_notes =  r"([1-2][0-9]{3}-[0-9]{2}-[0-9]{2}.*)\n\n([\s\S]*?)\n\n\n\n"
    notes_files = os.listdir(os.path.join(synthea_data_path, "notes"))
    with db_engine.connect() as db_connection:
        db_connection.execute(text("DROP TABLE IF EXISTS notes CASCADE"))
        for file_name in notes_files:
            patient_id = re.search(regex_id, file_name).group(2)
            all_notes = open(os.path.join(synthea_data_path, "notes", file_name)).read()
            notes = re.findall(regex_notes, all_notes)
            notes_df = pd.DataFrame(notes, columns=["date", "note_text"])
            notes_df["patient"] = patient_id
            notes_df["date"] = pd.to_datetime(notes_df["date"], format = "%Y-%m-%d")
            notes_df.to_sql("notes",
                            db_connection,
                            if_exists='append',
                            index=False,
                            method="multi",
                            chunksize=10000)
            db_connection.commit()


    index_columns = set(["udi", "id", "patient", "encounter", "provider",
                         "payer", "organization", "date", "start", "stop"])
    synthea_files  = {"notes": ["date"]}

    print("Adding indices")
    with db_engine.connect() as db_connection:
        for file_name, date_fields in synthea_files.items():
            print(f"... indexing {file_name}")
            # Read the column names
            df = pd.read_sql(f"SELECT * FROM {file_name} LIMIT 1", db_connection)

            # Add indexes and foreign keys
            # performance, since we're looking for complete duplicates
            for column in index_columns.intersection(set(list(df))):
                if column == "id":
                    print(f"  ... adding primary key for 'id'")
                    result = db_connection.execute(
                        text(f"""alter table public.{file_name} add constraint {file_name}_pk primary key (id);"""))
                else:
                    print(f"  ... adding index for '{column}'")
                    db_connection.execute(text(f"CREATE INDEX {file_name}_{column} ON {file_name} ({column})"))
        db_connection.commit()

if __name__ == '__main__':
    from argparse import ArgumentParser
    from dateutil import parser as date_parser
    parser = ArgumentParser(prog='load_notes_to_rdbms.py')
    parser.add_argument('--uid', default = 'postgres')
    parser.add_argument('--pwd', default='')
    parser.add_argument('--host', default='localhost')
    parser.add_argument('--db', default='syntheticmguh')
    parser.add_argument('--path', default='../output')
    parser.add_argument('--start', default='2010-01-01 00:00:00Z', type=lambda s: date_parser.parse(s))
    args = parser.parse_args()

    main(uid=args.uid,
         pwd=args.pwd,
         host=args.host,
         db=args.db,
         synthea_data_path=args.path,
         ehr_launch_date=args.start)
