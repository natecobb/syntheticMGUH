#!/bin/bash
# Simple script to sync Parquet files to Databricks Unity Catalog Volume

set -e

PARQUET_DIR="../output/parquet"
DATABRICKS_PATH="dbfs:/Volumes/mi2/i2l_syntheticmguh/raw/parquet"

echo "Syncing Parquet files to Databricks..."
echo "Source: $PARQUET_DIR"
echo "Destination: $DATABRICKS_PATH"
echo ""

# Check if parquet directory exists
if [ ! -d "$PARQUET_DIR" ]; then
    echo "Error: Parquet directory not found: $PARQUET_DIR"
    exit 1
fi

# Extract catalog and schema from DATABRICKS_PATH using regex
# Expected format: dbfs:/Volumes/CATALOG/SCHEMA/...
if [[ $DATABRICKS_PATH =~ dbfs:/Volumes/([^/]+)/([^/]+) ]]; then
    CATALOG="${BASH_REMATCH[1]}"
    SCHEMA="${BASH_REMATCH[2]}"
else
    echo "Error: Could not parse catalog/schema from DATABRICKS_PATH"
    exit 1
fi

echo "Detected catalog: $CATALOG"
echo "Detected schema: $SCHEMA"
echo ""

# Generate SQL import script
SQL_FILE="load_tables_databricks.sql"
echo "Generating SQL import script: $SQL_FILE"

cat > "$SQL_FILE" << EOF
-- Databricks SQL script to load Parquet files as tables
-- Auto-generated by sync_parquet_to_databricks.sh
-- Catalog: $CATALOG
-- Schema: $SCHEMA
-- Volume path: $DATABRICKS_PATH

-- Create schema if it doesn't exist
CREATE SCHEMA IF NOT EXISTS ${CATALOG}.${SCHEMA};

-- Create tables from Parquet files
EOF

# Loop through parquet files and generate CREATE TABLE statements
for parquet_file in "$PARQUET_DIR"/*.parquet; do
    if [ -f "$parquet_file" ]; then
        table_name=$(basename "$parquet_file" .parquet)
        # Remove dbfs: prefix for the SQL path
        sql_path="${DATABRICKS_PATH#dbfs:}"
        echo "CREATE OR REPLACE TABLE ${CATALOG}.${SCHEMA}.${table_name} AS SELECT * FROM parquet.\`${sql_path}/${table_name}.parquet\`;" >> "$SQL_FILE"
    fi
done

# Add verification queries
cat >> "$SQL_FILE" << EOF

-- Verify tables were created
SHOW TABLES IN ${CATALOG}.${SCHEMA};

-- Show row counts for each table
EOF

for parquet_file in "$PARQUET_DIR"/*.parquet; do
    if [ -f "$parquet_file" ]; then
        table_name=$(basename "$parquet_file" .parquet)
        echo "SELECT '${table_name}' as table_name, COUNT(*) as row_count FROM ${CATALOG}.${SCHEMA}.${table_name};" >> "$SQL_FILE"
    fi
done

echo "âœ“ SQL script generated: $SQL_FILE"
echo ""

# Sync all parquet files
echo "Copying Parquet files to Databricks..."
databricks fs cp "$PARQUET_DIR" "$DATABRICKS_PATH" --recursive --overwrite

# Upload SQL script to same location
echo ""
echo "Uploading SQL script to Databricks..."
databricks fs cp "$SQL_FILE" "${DATABRICKS_PATH}/${SQL_FILE}" --overwrite
rm $SQL_FILE

echo ""
echo "============================================================"
echo "Sync complete!"
echo "============================================================"
echo "Parquet files: $DATABRICKS_PATH"
echo "SQL script: ${DATABRICKS_PATH}/${SQL_FILE}"
echo ""
echo "To load tables in Databricks:"
echo "  1. Navigate to: ${DATABRICKS_PATH}/${SQL_FILE}"
echo "  2. Copy SQL contents and run in a SQL notebook"
echo "  Or download: databricks fs cp ${DATABRICKS_PATH}/${SQL_FILE} ./${SQL_FILE}"